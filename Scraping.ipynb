{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foursquare \n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas import DataFrame\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API keys - sign up on foursquare developers \n",
    "Client_id = 'NGQOAJVUOUUUJRNJVQBZT43ORT3ZLXSNXVINGWT2DYNPYVZQ'\n",
    "Client_secret = '0SYTBTQNVBGUH1QFOTH1FOAE5DN4XWQOD2145URFQB2TM5V3'\n",
    "client = foursquare.Foursquare(client_id=Client_id, client_secret=Client_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find all venues in area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lat/lng grid generating function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize = .002\n",
    "\"\"\"Find area of interest, give the bottom left corner of latitude/longitude for start, \n",
    "and top right corner for end, this function increases both lat and lon by the step size \n",
    "until it is larger than the limit. then creates unique pairs of lat/lon for the search\"\"\"\n",
    "\n",
    "def gridmaker(lat_start, lon_start, lat_end, lon_end):\n",
    "    lat = lat_start\n",
    "    lon = lon_start\n",
    "    #empty list to store values \n",
    "    lats = []\n",
    "    lons = []\n",
    "    lls = []\n",
    "    #getting all of the latitude within grid\n",
    "    while lat < lat_end: \n",
    "        lat = round(lat + stepsize, 6)\n",
    "        lats.append(lat)\n",
    "    #all longitude within grid\n",
    "    while lon < lon_end:  \n",
    "        lon = round(lon + stepsize, 6)\n",
    "        lons.append(lon)\n",
    "    # make pairs of all latitude longitudes \n",
    "    for latitude in lats: \n",
    "        for longitude in lons:\n",
    "            latitude = str(latitude)\n",
    "            longitude = str(longitude)\n",
    "            ll = latitude + \",\" +longitude\n",
    "            lls.append(ll) \n",
    "    return {'latitudes' : lats, 'longitudes' : lons, 'lls' : lls}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### venue generating function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_keys = [\"categories\",\"id\",\"location\",\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this function will return a table and create csv file that has the requested keys listed above, \n",
    "for every latitude longitude pair, the function calls, and return json, add it to a database, then drop duplicates\"\"\" \n",
    "def getvenues(lat_start, lon_start, lat_end, lon_end, save_to_name):\n",
    "    #getting grid of lat/lng\n",
    "    makegrid=gridmaker(lat_start, lon_start, lat_end, lon_end)\n",
    "    grid=makegrid['lls'] \n",
    "    \n",
    "    df = DataFrame()\n",
    "    # send in request to foursquare API \n",
    "    for ll in grid:\n",
    "        data = client.venues.search(params={'ll': ll})\n",
    "        d = DataFrame(data['venues'])[requested_keys]\n",
    "        df = df.append(d, ignore_index=True)\n",
    "    df = df.drop_duplicates('id')\n",
    "    df.to_csv(save_to_name, index=False)\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"how i obtained barcelona data\"\"\"\n",
    "#bottom rect\n",
    "grid_lower = getvenues(41.353250, 2.090730, 41.373091, 2.174500, 'venues_lower.csv')\n",
    "#middle rectangle of barcelona \n",
    "grid_mid = getvenues(41.373091, 2.093908, 41.402484, 2.221078, 'venues_middle.csv')\n",
    "#upper rectangle of barcelona \n",
    "grid_upper = getvenues(41.402484, 2.115555, 41.447677,2.258841, 'venues_upper.csv')\n",
    "#camden in london\n",
    "grid_london = getvenues(51.526159, -0.200429, 51.564691, -0.137379, \"venues_london.csv\")\n",
    "#read all of the csvs and join them into 1 \n",
    "v1=pd.read_csv('venues_upper.csv')\n",
    "v2=pd.read_csv('venues_middle.csv')\n",
    "v2=v2.drop(['Unnamed: 0'], axis=1)\n",
    "v3=pd.read_csv('venues_lower.csv')\n",
    "frames = [v1, v2, v3]\n",
    "venues = pd.concat(frames)\n",
    "venues = venues.drop_duplicates('id')\n",
    "venues.to_csv(\"venues_barcelona.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = pd.read_csv(\"venues_barcelona.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape directly from foursqure for userids that posted photos to a venue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this part requires you to create a csv of all of the venue ids from step 1, \n",
    "then run the scrape.sh file in bash. This will give you a folder of json files of photos for each venue.\n",
    "change the oauth token in the sh file to your own. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Organize user ids from json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ids(path_to_folder): \n",
    "    path_to_json = path_to_folder\n",
    "    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "    \n",
    "    #creating a nested list with venue ids and their respective visitor ids - what the list will contain \n",
    "    users = []\n",
    "    for file in json_files: \n",
    "        with open(path_to_json + \"/\" + file) as json_data:\n",
    "            d = json.load(json_data)\n",
    "            for k in d['response']['photos']['items']:\n",
    "                users.append([file[:-5], k['user']['id'], k['createdAt']])\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_folder = \"/Users/amyzha/Desktop/BGSE /Thesis/venue_users\"\n",
    "users = get_user_ids(step2_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize a csv with visitor time to a venue to be used in analysis\n",
    "userdf=pd.DataFrame(users)\n",
    "userdf.columns=['venue','user','time']\n",
    "userdf.to_csv(\"venue_visitors_withtime.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract userids to scrape for their info in the next step\n",
    "userids=list(set(userdf['user'].tolist()))\n",
    "DataFrame(list(set(userdf['user'].tolist()))).to_csv(\"userids.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scrape directly from foursquare for users' twitter contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this part requires you to use the userids.csv, \n",
    "then run the userdetails_scrape.sh file in bash. This will give you a folder of json files of user details for \n",
    "each user.\n",
    "change the oauth token in the sh file to your own. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Organize users' twitter ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested list of user id, contact, homecity \n",
    "def get_user_twitters(path): \n",
    "    path_to_json = path\n",
    "    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "    user_details = []\n",
    "    for file in json_files: \n",
    "        with open(path_to_json + \"/\" + file) as json_data:\n",
    "            d = json.load(json_data)\n",
    "            #only if twitter contact exists, then append to list of users \n",
    "            if ('twitter') in d['response']['user']['contact']: \n",
    "                user_details.append([file[:-5], d['response']['user']['contact']['twitter']])\n",
    "    #making a new list of twitter account names \n",
    "    twitter=[d[1] for d in user_details] \n",
    "    twitter=pd.DataFrame(twitter, columns=[\"twitter\"])\n",
    "    twitter.to_csv(\"twitterid.csv\", index=False, header=None)            \n",
    "    \n",
    "    return user_details\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new list of twitter account names \n",
    "step5_folder = \"/Users/amyzha/Desktop/BGSE /Thesis/userdetails\"\n",
    "userdetails=get_user_twitters(step5_folder)\n",
    "twitter=[d[1] for d in userdetails] \n",
    "pd.DataFrame(userdetails).to_csv('userdetails.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Scrape from Twitter for a user's friends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to do this we'll use the twint package to help us\n",
    "1. command line in the same directory: git clone https://github.com/twintproject/twint.git\n",
    "2. command line: pip3 install -r requirements.txt\n",
    "3. using the twitterid.csv, run the twitterscrapefollowers.sh & twitterscrapefollowing.sh file in the command line\n",
    "This will give you a folder of csv files of users followers and a folder of users following \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Store to Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient() # this stays the same and is the client \n",
    "db = client.gentrification #this is the database \n",
    "network_collection = db.users #this is a table in the database aka collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to help search previous lists\n",
    "def find_in_nested_list(mylist, char):\n",
    "    for sub_list in mylist:\n",
    "        if char in sub_list:\n",
    "            return mylist.index(sub_list)\n",
    "    raise ValueError(\"'{char}' is not in list\".format(char = char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match followers and following for actual \"friends\" only if the same value appears in both \n",
    "\n",
    "path_to_csv = '/Users/amyzha/Desktop/BGSE /Thesis/twitterfollowers'\n",
    "csv_files = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('.csv')]\n",
    "\n",
    "path_to_csv1 = '/Users/amyzha/Desktop/BGSE /Thesis/twitterfollowings'\n",
    "\n",
    "for file in csv_files: \n",
    "    #list of followers\n",
    "    with open(path_to_csv + \"/\" + file, \"r\") as csv_data1:\n",
    "        reader = csv.reader(csv_data1)\n",
    "        data_followers = list(reader)\n",
    "        followers = [item for sublist in data_followers for item in sublist]\n",
    "    #list of following\n",
    "    with open(path_to_csv1 + \"/\" + file, \"r\") as csv_data2:\n",
    "        reader1 = csv.reader(csv_data2)\n",
    "        data_following = list(reader1)\n",
    "        following = [item for sublist in data_following for item in sublist]\n",
    "    #find the matches \n",
    "    friends = list(set(followers) & set(following))\n",
    "    foursquare_friends = [x for x in friends if x in twitter] #we only include friends that are also in the foursquare network and have visited a venue\n",
    "    index = find_in_nested_list(userdetails, file[:-4]) #finding the userid\n",
    "    userid = userdetails[index][0] #finding the userid\n",
    "    #hometown = userdetails[index][2] #finding hometown\n",
    "    venues=list(set([x[0] for x in users if x[1] == userid])) #finding all venues \n",
    "    \n",
    "    #store in mongoDB \n",
    "    record = {\n",
    "        \"user\": {\n",
    "        \"userId\" : userid,\n",
    "        \"twitterHandle\" : file[:-4] #,\n",
    "        #\"hometown\" : hometown\n",
    "    },\n",
    "    \"friends\": foursquare_friends,\n",
    "    \"venues\": venues  \n",
    "    }\n",
    "\n",
    "    network_collection.insert_one(record) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store to MongoDB for London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient() # this stays the same and is the client \n",
    "db = client.gentrification #this is the database \n",
    "networklondon_collection = db.network_london1 #this is a table in the database aka collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match followers and following for actual \"friends\" only if the same value appears in both \n",
    "\n",
    "path_to_csv = '/Users/amyzha/Desktop/BGSE /Thesis/twitterfollowers_london'\n",
    "csv_files = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('.csv')]\n",
    "\n",
    "path_to_csv1 = '/Users/amyzha/Desktop/BGSE /Thesis/twitterfollowings_london'\n",
    "\n",
    "for file in csv_files: \n",
    "    #list of followers\n",
    "    with open(path_to_csv + \"/\" + file, \"r\") as csv_data1:\n",
    "        reader = csv.reader(csv_data1)\n",
    "        data_followers = list(reader)\n",
    "        followers = [item for sublist in data_followers for item in sublist]\n",
    "    #list of following\n",
    "    with open(path_to_csv1 + \"/\" + file, \"r\") as csv_data2:\n",
    "        reader1 = csv.reader(csv_data2)\n",
    "        data_following = list(reader1)\n",
    "        following = [item for sublist in data_following for item in sublist]\n",
    "    #find the matches \n",
    "    friends = list(set(followers) & set(following))\n",
    "    foursquare_friends=[f for f in friends if f in twitter_london]\n",
    "    index = find_in_nested_list(userdetails_london, file[:-4]) #finding the userid\n",
    "    userid = userdetails_london[index][0] #finding the userid\n",
    "    #hometown = userdetails_london[index][2] #finding hometown\n",
    "    venues=[x[0] for x in users_london.values.tolist() if userid == x[1]] #finding all venues \n",
    "    #store in mongoDB \n",
    "    record = {\n",
    "    \"user\": {\n",
    "        \"userId\" : userid,\n",
    "        \"twitterHandle\" : file[:-4],\n",
    "        #\"hometown\" : hometown\n",
    "    },\n",
    "    \"friends\": foursquare_friends,\n",
    "    \"venues\": venues  \n",
    "    }\n",
    "    print(file)\n",
    "    #networklondon_collection.insert_one(record) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
